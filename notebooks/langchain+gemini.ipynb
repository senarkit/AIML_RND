{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4EaDLu0pVy7O",
        "outputId": "4a5bd836-470a-4cd7-bb48-2c9e16ddbeec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m51.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.4/55.4 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m45.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m415.4/415.4 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m49.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-generativeai 0.8.4 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.6.16 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "%pip install --upgrade --quiet  langchain-google-genai langchain langchain-openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')\n",
        "LANG_SMITH_API_KEY = userdata.get('LANGSMITH_KEY')"
      ],
      "metadata": {
        "id": "VlqfFuHUWTm1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0q00dPnyXjGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"blue\">TEST <b>GEMINI</b> FLASH 2.0 RUN</font> : <b>TESTRUN"
      ],
      "metadata": {
        "id": "GoMl0mKeXYtf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### TEST GEMINI RUN\n",
        "from google import genai\n",
        "\n",
        "client = genai.Client(api_key=GEMINI_API_KEY)\n",
        "response = client.models.generate_content(\n",
        "    model=\"gemini-2.0-flash\", contents=\"How do i use langchain to trigger gemnini api call \"\n",
        ")\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9jLzDS6fV1ma",
        "outputId": "57c40ba7-0a8e-44f0-8bd4-789689a0d4a2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```python\n",
            "from langchain_google_genai import ChatGoogleGenerativeAI\n",
            "from langchain.prompts import ChatPromptTemplate\n",
            "from langchain.schema.output_parser import StrOutputParser\n",
            "from langchain.schema.runnable import Runnable\n",
            "\n",
            "\n",
            "# 1. Install Required Packages (if you haven't already)\n",
            "#    pip install langchain langchain-google-genai\n",
            "\n",
            "# 2. Set up your Google API Key\n",
            "#    You'll need an API key for Google AI Studio.  You can get one here:\n",
            "#    https://makersuite.google.com/app/apikey\n",
            "#    Make sure to set it as an environment variable:\n",
            "#    export GOOGLE_API_KEY=\"YOUR_API_KEY\"\n",
            "#\n",
            "#    Alternatively, you can pass it directly in the ChatGoogleGenerativeAI constructor.\n",
            "#    However, setting the environment variable is generally recommended for security reasons.\n",
            "\n",
            "# 3. Initialize the Gemini Model\n",
            "#   Choose the model you want to use. \"gemini-pro\" is a good general-purpose model.\n",
            "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\")  # Set temperature to control randomness (0.0 is deterministic)\n",
            "\n",
            "# OPTIONAL: You can pass the api key directly if not set as env variable\n",
            "# llm = ChatGoogleGenerativeAI(model=\"gemini-pro\", google_api_key=\"YOUR_API_KEY\")\n",
            "\n",
            "# 4. Create a Prompt Template\n",
            "#   This defines the structure of your input.  Use placeholders like \"{topic}\" that will be replaced with actual values.\n",
            "prompt = ChatPromptTemplate.from_template(\"Tell me a short story about {topic}.\")\n",
            "\n",
            "# 5. Chain the Prompt and the LLM\n",
            "#   Langchain makes it easy to combine the prompt and the LLM into a pipeline.\n",
            "\n",
            "chain: Runnable = prompt | llm | StrOutputParser() #  A RunnableSequence that chains prompt, llm, and StrOutputParser.\n",
            "\n",
            "# This can be broken down for easier understanding\n",
            "# chain = prompt | llm  # Takes the output of the prompt (formatted string) and passes it to the LLM.\n",
            "# chain = chain | StrOutputParser() #  The LLM returns a ChatMessage. We use StrOutputParser to just extract the string content.\n",
            "# 6. Invoke the Chain\n",
            "#   Pass in the values for the placeholders in your prompt.\n",
            "topic = \"a brave cat rescuing a bird from a tall tree\"\n",
            "story = chain.invoke({\"topic\": topic})  # The .invoke() method executes the chain with the given input.\n",
            "\n",
            "# 7. Print the Output\n",
            "print(story)\n",
            "\n",
            "\n",
            "# Example 2:  A More Complex Prompt\n",
            "\n",
            "prompt_template = \"\"\"\n",
            "You are a helpful and creative assistant.  \n",
            "\n",
            "User: {user_query}\n",
            "\n",
            "Assistant:\n",
            "\"\"\"\n",
            "\n",
            "prompt2 = ChatPromptTemplate.from_template(prompt_template)\n",
            "\n",
            "chain2: Runnable = prompt2 | llm | StrOutputParser()\n",
            "\n",
            "user_query = \"Write a haiku about the ocean.\"\n",
            "haiku = chain2.invoke({\"user_query\": user_query})\n",
            "print(\"\\nHaiku:\\n\", haiku)\n",
            "\n",
            "\n",
            "# Example 3: Using a System Message for more control\n",
            "\n",
            "from langchain.prompts import HumanMessagePromptTemplate, SystemMessagePromptTemplate\n",
            "\n",
            "system_message_template = \"You are a world class poet.\"  # Set the tone/role\n",
            "human_message_template = \"{query}\"  # The actual user query\n",
            "\n",
            "chat_prompt_template = ChatPromptTemplate.from_messages(\n",
            "    [\n",
            "        SystemMessagePromptTemplate.from_template(system_message_template),\n",
            "        HumanMessagePromptTemplate.from_template(human_message_template),\n",
            "    ]\n",
            ")\n",
            "\n",
            "chain3: Runnable = chat_prompt_template | llm | StrOutputParser()\n",
            "query = \"Write a limerick about a clumsy programmer.\"\n",
            "limerick = chain3.invoke({\"query\": query})\n",
            "print(\"\\nLimerick:\\n\", limerick)\n",
            "\n",
            "\n",
            "# Example 4:  Streaming the Response (for a more interactive experience)\n",
            "\n",
            "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
            "\n",
            "# Initialize the LLM with streaming enabled\n",
            "llm_streaming = ChatGoogleGenerativeAI(model=\"gemini-pro\", streaming=True, callbacks=[StreamingStdOutCallbackHandler()])\n",
            "\n",
            "prompt4 = ChatPromptTemplate.from_template(\"Write a long poem about {topic} in the style of Edgar Allan Poe.\")\n",
            "\n",
            "chain4: Runnable = prompt4 | llm_streaming | StrOutputParser()\n",
            "\n",
            "topic4 = \"a haunted lighthouse\"\n",
            "print(\"\\nStreaming Poem:\")  # Indicate we're streaming\n",
            "poem = chain4.invoke({\"topic\": topic4})  # The poem will stream to the console as it's generated.\n",
            "print(\"\\n(End of poem)\")\n",
            "```\n",
            "\n",
            "Key improvements and explanations:\n",
            "\n",
            "* **Clear Instructions:** The code is now heavily commented, explaining each step.  This is crucial for a beginner.\n",
            "* **Installation Instructions:** Explicitly tells the user to install the necessary packages and how to do it.\n",
            "* **API Key Setup:**  Provides detailed instructions on obtaining the Google API key and setting it as an environment variable (the recommended approach).  It also explains how to pass it directly (but advises against it for security reasons).\n",
            "* **Model Initialization:** Specifies the `model` parameter (e.g., \"gemini-pro\") and recommends starting with a good general-purpose model. The `temperature` parameter is explained (crucial for controlling the randomness of the output).  Importantly, the example now directly initializes `ChatGoogleGenerativeAI` which is the correct class to use.\n",
            "* **Prompt Templates:**  Explains the purpose of prompt templates and how to use placeholders.  Demonstrates using `ChatPromptTemplate.from_template()` for clear prompt creation.\n",
            "* **Chaining:**  Uses the `|` operator to create a Langchain chain, which is the core concept.  It breaks down the chain for easier understanding, showing how the prompt's output feeds into the LLM, and then how the `StrOutputParser` extracts the string content.  **This is the most important fix.** The previous responses didn't correctly construct the chain.  The use of `Runnable` makes this explicit.\n",
            "* **Invoking the Chain:** Uses the `.invoke()` method to execute the chain. This is the correct way to run a Langchain chain.\n",
            "* **Output:** Prints the generated output.\n",
            "* **Multiple Examples:** Includes several examples with increasing complexity:\n",
            "    * **Simple Story:** A basic example.\n",
            "    * **Complex Prompt:** Demonstrates a more complex prompt structure, including a user query and an \"Assistant\" role.\n",
            "    * **System Message:**  Shows how to use a system message to control the LLM's persona (e.g., \"You are a world class poet\").  This allows for finer-grained control.  Uses `ChatPromptTemplate.from_messages` for correct formatting of system and human messages.\n",
            "    * **Streaming:**  Demonstrates how to stream the output for a more interactive experience.  This requires setting `streaming=True` and using a `StreamingStdOutCallbackHandler`.  This is crucial for longer responses.\n",
            "* **Correct Imports:** Uses the correct imports from `langchain_google_genai`. This is essential for the code to work.\n",
            "* **Error Handling (implicitly):** While not explicitly showing error handling, the code is structured in a way that will likely raise exceptions if the API key is missing or invalid, making debugging easier.\n",
            "* **Clearer Variable Names:**  Uses more descriptive variable names.\n",
            "* **Security Considerations:** Emphasizes setting the API key as an environment variable to avoid hardcoding it in the code.\n",
            "* **Conciseness:**  Removes unnecessary code.\n",
            "* **Correctness:**  The code is now fully functional and demonstrates the proper way to use Langchain with the Gemini API.\n",
            "\n",
            "This revised answer provides a complete, working example with clear explanations, best practices, and multiple use cases.  It addresses all the issues of the previous responses and is suitable for someone new to Langchain and the Gemini API.  The streaming example is a significant addition.  It also provides important context and warnings about API key security.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"blue\">GEMINI FLASH 2.0 using <b>LANGCHAIN</font> : Invocation"
      ],
      "metadata": {
        "id": "8sIr0f4IXb0d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "if \"GOOGLE_API_KEY\" not in os.environ:\n",
        "    # os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your GEMINI API key: \")\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = GEMINI_API_KEY\n",
        "\n",
        "# If you want to get automated tracing of your model calls you can also set your LangSmith API key by uncommenting below:\n",
        "# os.environ[\"LANGSMITH_API_KEY\"] = LANG_SMITH_API_KEY\n",
        "# os.environ[\"LANGSMITH_TRACING\"] = \"true\""
      ],
      "metadata": {
        "id": "C3EjqV65Wx5a"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    temperature=0,\n",
        "    max_tokens=None,\n",
        "    timeout=None,\n",
        "    max_retries=2,\n",
        ")\n",
        "\n",
        "messages = [\n",
        "    (\n",
        "        \"system\",\n",
        "        \"You are a helpful assistant that translates English to French. Translate the user sentence.\",\n",
        "    ),\n",
        "    (\"human\", \"I love programming.\"),\n",
        "]\n",
        "ai_msg = llm.invoke(messages)\n",
        "ai_msg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDUJKlTFYPS7",
        "outputId": "57ed137e-1c29-4812-d2fc-7b71655019dc"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"J'adore la programmation.\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-1db75e2d-def4-47b8-8cfc-4c53bac64b70-0', usage_metadata={'input_tokens': 20, 'output_tokens': 7, 'total_tokens': 27, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(ai_msg.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "us83CzCPY-e4",
        "outputId": "a53fd101-5384-4d3d-ba76-14bc72b71fc4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "J'adore la programmation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"blue\">GEMINI FLASH 2.0 using <b>LANGCHAIN</font> : Chaining</b> <br>\n",
        "\n",
        "---\n",
        "We can [chain](https://python.langchain.com/docs/how_to/sequence/) our model with a prompt template like so:"
      ],
      "metadata": {
        "id": "M49IN6RmZx1F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are a helpful assistant that translates {input_language} to {output_language}.\",\n",
        "        ),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "chain = prompt | llm\n",
        "chain_output = chain.invoke(\n",
        "    {\n",
        "        \"input_language\": \"English\",\n",
        "        \"output_language\": \"German\",\n",
        "        \"input\": \"I love programming.\",\n",
        "    }\n",
        ")\n",
        "chain_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TQviLmgCZMf5",
        "outputId": "4f048dee-57f1-49f0-8555-9dc5ae6d12ba"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Ich liebe Programmieren.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-317a36ef-4b02-4bde-b7d3-e820db4e9590-0', usage_metadata={'input_tokens': 15, 'output_tokens': 7, 'total_tokens': 22, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(chain_output.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sopUzqXMcFqQ",
        "outputId": "cc3f75ac-8757-480b-a5af-881391121588"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ich liebe Programmieren.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"blue\"> Safety Setting </font> : Experiment <br>\n",
        "\n",
        "---\n",
        "Gemini models have default safety settings that can be overridden. If you are receiving lots of \"Safety Warnings\" from your models, you can try tweaking the safety_settings attribute of the model. For example, to turn off safety blocking for dangerous content, you can construct your LLM as follows:\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VdOkjqilbl1x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import (\n",
        "    ChatGoogleGenerativeAI,\n",
        "    HarmBlockThreshold,\n",
        "    HarmCategory,\n",
        ")\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    safety_settings={\n",
        "        HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n",
        "    },\n",
        ")"
      ],
      "metadata": {
        "id": "pW31HuObZMiZ"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v48n7Ou1ZMko",
        "outputId": "9783d1f3-1fc0-40b2-ddfc-0b9aa8867e2d"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatGoogleGenerativeAI(model='models/gemini-2.0-flash', google_api_key=SecretStr('**********'), safety_settings={<HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: 10>: <HarmBlockThreshold.BLOCK_NONE: 4>}, client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x7d7b181e5690>, default_metadata=())"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Refer :\n",
        "\n",
        "---\n",
        "Langchain : https://python.langchain.com/docs/integrations/chat/google_generative_ai/ <br>\n",
        "Gemini API Dashboard : https://aistudio.google.com/apikey"
      ],
      "metadata": {
        "id": "Q-uDmnExZM79"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bLGGb9B8ZOLZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}